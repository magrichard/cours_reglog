---
title: "Régression Logistique"
subtitle: "Séance 1"
author: "Florent Chuffart & Magali Richard (d'après le cours de Lydiane Agier)"
date: "13 Novembre 2018"
output: slidy_presentation
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, width = 75)
knitr::opts_chunk$set(echo = TRUE, fig.align='center', dev='png', dpi = 95, out.width = "100%")
```

---

## Evaluation

 - individuelle sur un jeu de données aprés 2 séances de "Régression Logistique" 
 - data challenge à la fin des séances "Régression Logistique" et "Survie" (23 janvier 2019)

## Pré-requis
 
 - R https://cran.r-project.org 
 - RStudio https://www.rstudio.com

## Cours 

- https://github.com/magrichard/cours_reglog

---

# Plan

I) Rappel (régression linéaire)
II) Introduction à la regression logistique
III) Formalisme et principe (modèle logit)
IV) Estimation des coefficients beta

---

## I.  Régression linéaire (rappel) 

Y est expliquée (modélisée) par  les variables explicatives $X= (X_1,X_2,...,X_p)$.

Si $p=1$, c’est une fonction affine de X.

Modèle : $$E(Y)  = \beta X= \beta_0 + \beta_1X_1 + … + \beta_pX_p$$

avec $\beta=(\beta_0, \beta_1, …, \beta_p)$ estimé par les moindres carrées 

$$\beta = argmin(\sum_{i=1}^{n} {(y_i - \beta_0 - \beta_1x_{1,i} - ... - \beta_px_{p,i})^2)}$$ 

Sous l’hypothèse de normalité des résidus, l’estimateur des moindres carrés est équivalent à l’estimateur du maximum de vraisemblance pour la régression linéaire :

- La valeur estimée (ajustée) de Y est $\widehat{y}_i =  \widehat{\beta}_0 + \widehat{\beta}_1X_1 +  … + \widehat{\beta}_pX_p$
- Les résidus sont estimés par $e_i = y_i -\widehat{y}_i$
- La variance du terme d’erreur est estimée par $s^2 = \frac{1}{n-2} \sum_{i=1}^{n}{e_i^2}$


---

## I) Données : data_nutri

```{r}
d = read.table("data/data_nutri.csv", header=TRUE, sep=",", row.names = 1)
d$sex = as.factor(d$sex)
DT::datatable(d, width = "100%")
# head(d)
```

---

## I) Exemple 1

```{r}
# README RStudio config, uncheck: # preferences > R Markdown > show output inline for... 
layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
plot(d$taille, d$poids, main="poids~taille")
## Model
# Y~X
# E(Y) = b.X
# E(Y) = b_0 + b_1.X
# Y_i = b_0 + b_1.X_i + e_i
m = lm(d$poids~d$taille)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x + a
# residuals
m$residuals
arrows(d$taille, d$poids, d$taille, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("topleft",c("regression line", "residuals"), col=c(2,4), lty=1, cex=.8)
```

---

## I) Exemple 2

```{r}
layout(matrix(1:2, 1), respect=TRUE)
plot(d$sex, d$poids, main="poids~sex", xlab="sex", ylab="poids")
## Model
# Y~X
# E(Y) = b.X
# E(Y) = b_0 + b_1.X
# Y_i = b_0 + b_1.X_i + e_i
m = lm(d$poids~d$sex)
m$coefficients
abline(h=m$coefficients[[1]], col=2)
abline(h=m$coefficients[[1]] + m$coefficients[[2]], col=4)
legend("bottomright",c("b_0", "b_0+b_1"), col=c(2,4), lty=1, cex=.8)

plot(d$sex, d$poids, main="poids~sex", xlab="sex", ylab="poids")
points(as.numeric(d$sex), d$poids)
x = jitter(as.numeric(d$sex), 0.5)
points(x, d$poids)
arrows(x, d$poids, x, d$poids-m$residuals, col=adjustcolor(4, alpha.f=0.5), length=0.1)
legend("bottomright",c("residuals"), col=c(4), lty=1, cex=0.6)

# t.test
# 1 null hypothesis
# H_0: p_h==_f
# H_1: p_h!=p_f
# 2 risk
# alpha = 5%
# 3 test hypothesis
# residuals~N?
shapiro.test(d[d$sex=="Femme",]$poids)
shapiro.test(d[d$sex=="Homme",]$poids)
# -> OK
# 4 t.test
t.test(d[d$sex=="Femme",]$poids, d[d$sex=="Homme",]$poids)
anova(m)
# 5 conclusion
```

---

## II) Introduction à la regression logistique : objectifs

Objectif : Modéliser une **variable binaire** en fonction d’une ou plusieurs autres covariables (quali ou quanti)
$$Y \sim X$$
$$E(Y|X) \sim X$$

Exemple de variable à expliquer : 
  
  - Le sexe en fonction du poids
  - Maladie coronarienne en fonction d’HTA et cholestérol
  - Survenue de cancer en fonction d’expositions chimiques
  - Consommation d’un bien selon variables sociodémographiques
  - Risque d’accoucher d’un bébé de faible poids (<2500g) en fonction de l’âge de la mère, du poids, du tabagisme.

---

## II) Exemple 3

```{r}
layout(matrix(1:2, 1), respect=TRUE)
s = as.numeric(d$sex) - 1
plot(d$poids, s, main="sex~poids", xlab="poids", ylab="sex")
## Model
# Y~X
# E(Y) = b.X
# E(Y) = b_0 + b_1.X
# Y_i = b_0 + b_1.X_i + e_i
m = lm(s~d$poids)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2, lwd=2) # /!\ y = b.x + a
plot(d$poids, s, main="sex~poids", xlab="poids", ylab="sex")
# residuals
arrows(d$poids, s, d$poids, s-m$residuals, col=adjustcolor(4, alpha.f=0.2), length=0.1, lwd=2)
legend("bottomright",c("regression line", "residuals"), col=c(2,4), lty=1, cex=0.6)
```

---

## III) Formalisme : 

### Variable de Bernoulli

- Toute variable binaire peut être **codée en 0/1**, et est alors considérée comme une Variable de Bernoulli de distribution:
 
$$P(Y=1)=p$$
$$P(Y=0)=1-p$$ 
$$P(Y=k)=p^k(1-p)^{1-k}, k \in \{0,1\}$$ 


### Principe

 - On pose Y la variable binaire,
 - X le vecteur des covariables/variables explicatives (qualitatives ou quantitatives)
 - On veut modéliser $E(Y|X) = f(X)$ avec $E(Y|X) = P(Y=1|X)$ noté $\pi (X)$

---

## III) Formalisme : 

### Fonction logit

La fonction *logit* est définie de $[0,1]$ dans $R$ par:
$$	logit:[0,1]	\rightarrow	R $$
$$	x	\rightarrow	logit(x) = log(\frac{x}{1-x}) $$
$$ \lim_{x\to0} logit(x) = -\infty $$
$$ \lim_{x\to1} logit(x) = +\infty $$

La fonction *logit* s’inverse :
$$	logit^{-1}: R 	\rightarrow	[0,1] $$
$$	x	\rightarrow	logit^{-1}(x) = \frac{1}{1 + e^{-x}} $$



---

## III) Exemple 4 : la fonction logit

```{r}
# logit: [0,1] -> R
#            p -> log(p/1-p)
# logit(p) = log(p/1-p)
x = 0:100/100
layout(matrix(1:6, 2, byrow=TRUE), respect=TRUE)
plot(x, log(x/(1-x)), main="logit")
plot(log(x/(1-x)), x, main="logit^-1")
```









---

## III) Formalisme : le modèle logistique

On utilise donc le modèle logistique:
    $$logit(E(Y|X)) = \beta X$$
    $$logit(P(Y=1|X))= \beta X$$ 
    $$logit(\pi (X))= \beta X$$ 
    $$\pi (X)= logit^{-1}(\beta X)$$ 

Comme en régression linéaire, l’objet de cette modélisation est *d’estimer les coefficients $\beta$*


---

## III) Exemple 5 : le modèle logistique

```{r}

layout(matrix(1:2, 1), respect=TRUE)
plot(d$poids, s, main="sex~poids", xlab="poids", ylab="sex")
# P(Y=1|X) = logitinv(a + b.x)
m = glm(s~d$poids, family = binomial(logit))
m$coefficients
logitinv = function(x) 1/(1 + exp(-x))
x = min(d$poids):max(d$poids)
lines(x, logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x), col=2, lwd=2)
legend("bottomright", "logit(Y)=b.X", col=2, lty=1, cex=0.6)
plot(d$poids, s, main="sex~poids", xlab="poids", ylab="sex")
py1x = function(t,m) {
  x = m$coefficients[[1]] + m$coefficients[[2]]*t
  1/(1 + exp(-x))
}
arrows(d$poids, s, d$poids, py1x(d$poids,m), col=adjustcolor(4, alpha.f=0.2), length=0.05, lwd=3)
legend("bottomright","P(Y|X)", col=4, lty=1, cex=0.6)
```




---

## IV) Estimation : la vraisemblance

- **Définition** : Probabilité d’observer un évènement si le modèle envisagé est vrai

- La fonction de vraisemblance dépend de la distribution de $Y$

- On note les covariables globales théoriques: $X=(X_1,...,X_p)$ et observées : $x=(x_1,...,x_p)$. Les covariables individuelles observées sont notées : $x_i=(x_{1i},...,x_{pi})$

- Le modèle conditionnel de $Y$ sachant $X$ est: $E(Y|X = x) \sim B(\pi(x))$

- Pour l’observation $i$, la contribution à la vraisemblance est donc:
$$l(x_i, y_i) = P(y_i = 1|x_i)^{y_i} (1-P(y_i=1|x_i))^{1-y_i} = \pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i} $$


---

## IV) Estimation : les coefficients (1/2)

- Le modèle s’écrit:  $logit(E(Y|X_1,...,X_p)) = \beta_0 + \beta_1X_1+...+\beta_pX_p$
Les paramètres $\beta = (\beta_0,\beta_1, ..., \beta_p)$ sont inconnus

- On estime $\beta$ par **maximum de vraisemblance**.

- La vraisemblance conditionnelle est:
$$\prod_{i=1}^{n} l(x_i, y_i) = \prod_{i=1}^{n} \pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i} = \prod_{i=1}^{n} \Big(\frac{\pi(x_i)}{1-\pi(x_i)}\Big)^{y_i}(1-\pi(x_i))$$


---

## IV) Estimation : les coefficients (2/2)


- La vraisemblance conditionnelle est:
$$\prod_{i=1}^{n} l(x_i, y_i) = \prod_{i=1}^{n} \pi(x_i)^{y_i}(1-\pi(x_i))^{1-y_i} = \prod_{i=1}^{n} \Big(\frac{\pi(x_i)}{1-\pi(x_i)}\Big)^{y_i}(1-\pi(x_i))$$
- En passant au logarithme, on a :

$$log(L_n(\beta)) = \sum_{i=1}^{n}\Big[ y_i * log \Big(\frac{\pi(x_i)}{1-\pi(x_i)}\Big) + log(1-\pi(x_i))\Big] $$

- Cette fonction contient des valeurs observées $y_i$ et des valeurs prédites $\pi(x_i)$ qui dépendent de $\beta$

- On maximise cette fonction en trouvant la valeur de $\beta$ *pour laquelle la dérivée (par rapport à $\beta$) est nulle*  : $L_n'(\beta) = 0$, definissant ainsi : 
$$\widehat{\beta}_n = argmax L_n(\beta)$$

---

## IV) Estimation : remarques

- Il n’existe pas de **solution analytique** de $L_n'(\beta) = 0$

- L’estimation de  $\widehat{\beta}$  est donc fait **par approximation** (d’où des résultats potentiellement différents selon l’algorithme), avec souvent la démarche suivante:
1) Le logiciel propose une première valeur pour $\beta$ et calcule la vraisemblance associée
2) Il cherche de nouvelles valeurs «plausibles» pour $\beta$ et voit si elles améliorent la vraisemblance
3) Et réitère cela jusqu’à atteindre une condition, eg. la différence de vraisemblance est <0.00001

- L’estimateur $\widehat{\beta}$ est estimé par le modèle, et non pas observé: sa valeur **dépend du modèle utilisé et de la validité de celui-ci**.

---

## IV) Estimation : Propriétés de l'estimateur (1/2)

- Asymptotiquement (i.e quand n tend vers l'infini), l’estimateur du maximum de vraisemblance:
1) existe et est unique
2) est sans biais  (i.e. il tend vers sa valeur réelle)
3) est de distribution normale
4) est efficace (i.e. de variance minimale parmi tous les estimateurs sans biais obtenus avec d’autres méthodes).

- Ainsi: $\lim_{n\to\infty} \sqrt{n}(\widehat{\beta}-\beta) \rightarrow N(0,\Sigma^{-1})$ avec $\Sigma^{-1}$ la matrice de variance-covariance de $\beta$ 

---

## IV) Estimation : Propriétés de l'estimateur (2/2)


- On en déduit les **intervalles de confiance** pour $\beta_k$:

$$ IC(\widehat{\beta}) = \Big[\widehat{\beta_k} -t_{1-\alpha/2,n-2} * \sqrt{\widehat{var}(\widehat{\beta_k})} ; \widehat{\beta_k}+t_{1-\alpha/2,n-2} * \sqrt{\widehat{var}(\widehat{\beta_k})} \Big]$$

Avec  $t_{1-\alpha/2,n-2}$ le quantile de niveau $1-\alpha/2$ de la loi de student à (n-2) degrés de liberté 

En pratique approximé dès que $n>30$ par $u1-\alpha/2$ le quantile de niveau $1-\alpha/2$ de la loi normale

---

## IV) Estimation : Valeurs prédites et résidus

A partir de $\widehat{\beta}$ , on peut calculer:

1) les **valeurs prédites** : $\pi(X)$ 

i.e. la probabilité estimée de $Y=1$ pour chaque individu en fonction de ses caractéristiques

2) les **résidus** : $\epsilon = Y - \widehat{\pi}(X)$

Ceux-ci peuvent servir à estimer l‘adéquation du modèle aux données (vus plus loin)



---

# Exemple 6
```{r}

m = glm(s~d$poids+d$taille+d$age, family = binomial(logit))
m$coefficients
summary(m)

layout(matrix(1:2, 1, byrow=TRUE), respect=TRUE)
plot(d$taille, d$poids, main="poids~taille", col=s+1)
## Model
# Y~X
# E(Y) = b.X
# E(Y) = b_0 + b_1.X
# Y_i = b_0 + b_1.X_i + e_i
m = lm(d$poids~d$taille)
m$coefficients
abline(a=m$coefficients[[1]], b=m$coefficients[[2]], col=2) # /!\ y = b.x 



layout(matrix(1:2, 1), respect=TRUE)
plot(d$poids, s, main="sex~poids", xlab="poids", ylab="sex")
# P(Y=1|X) = logitinv(a + b.x)
m = glm(s~d$poids, family = binomial(logit))
m$coefficients
logitinv = function(x) 1/(1 + exp(-x))
x = min(d$poids):max(d$poids)
lines(x, logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x), col=2, lwd=2)
py1x = function(t,m) {
  x = m$coefficients[[1]] + m$coefficients[[2]]*t
  1/(1 + exp(-x))
}
arrows(d$poids, s, d$poids, py1x(d$poids,m), col=adjustcolor(4, alpha.f=0.2), length=0.05, lwd=3)
legend("bottomright","P(Y|X)", col=4, lty=1, cex=0.6)


x = min(d$poids):max(d$poids)
plot(x,log(logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x) / (1-logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x))))
plot(x,logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x) / (1-logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x)))
plot(x,exp(m$coefficients[[1]] + m$coefficients[[2]]*x))

x = 72
logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x) / (1-logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x))





plot(d$taille, s, main="sex~taille", xlab="taille", ylab="sex")
# P(Y=1|X) = logitinv(a + b.x)
m = glm(s~d$taille, family = binomial(logit))
m$coefficients
logitinv = function(x) 1/(1 + exp(-x))
x = min(d$taille):max(d$taille)
lines(x, logitinv(m$coefficients[[1]] + m$coefficients[[2]]*x), col=2, lwd=2)
py1x = function(t,m) {
  x = m$coefficients[[1]] + m$coefficients[[2]]*t
  1/(1 + exp(-x))
}
arrows(d$taille, s, d$taille, py1x(d$taille,m), col=adjustcolor(4, alpha.f=0.2), length=0.05, lwd=3)
legend("bottomright","P(Y|X)", col=4, lty=1, cex=0.6)


```
